# Target Data Pipeline

This repository contains a modular, config-driven data curation and processing pipeline for biomedical entities such as genes, transcripts, proteins, pathways, and diseases. It is designed to support reproducibility, automation, and downstream graph modeling.

## ğŸ“ Structure

```bash
src/                    # Core processing scripts
â”œâ”€â”€ publicdata/         # Domain-specific modules (targets, drugs, etc.)
â”‚   â””â”€â”€ target_data/    # e.g., ensembl_data.py, ncbi_data.py...
data/                   # Input/output
â”œâ”€â”€ raw/                # Downloaded files (ignored in Git)
â”œâ”€â”€ cleaned/            # Final curated outputs
â”œâ”€â”€ semi/               # Intermediate/merged
â”œâ”€â”€ logs/               # Metadata & provenance
reports/                # JSON/CSV summaries (included in Git)
tests/                  # Pytest unit tests per module
workflows/              # Snakemake workflows, cron scripts
scripts/                # CLI entry points like main.py
```

## ğŸ§ª Testing

Run all tests using `pytest`:

```bash
pytest tests/
```

## ğŸ› ï¸ Usage

```bash
python src/scripts/main.py TARGETS --all
```
or 
```bash
python main.py TARGETS --modules ensembl ncbi
```
Or use Snakemake:

```bash
snakemake -s workflows/Snakefile --cores 4
```

## ğŸ“¦ Dependencies

Install with pip:

```bash
pip install -r requirements.txt
```

Or via conda:

```bash
conda env create -f environment.yml
```

## ğŸ“Š Outputs
- Cleaned CSVs in `data/cleaned/`
- Summary reports in `reports/`
- Metadata logs in `data/logs/`

## ğŸ“… Automate
Schedule with cron or run full DAG via Snakemake.

---

Â© 2025 NCATS_IFX
