# Entity Resolver Pipeline

This repository contains a modular, config-driven data curation and processing pipeline for biomedical entities such as genes, transcripts, proteins, pathways, and diseases. It is designed to support reproducibility, automation, and downstream graph modeling.

## ğŸ› ï¸ Getting Started

### 0) Setup

```bash
bash setup.sh
pip install -r requirements.txt
python src/code/main.py --help
```
## ğŸ“ Structure
```
config/
  â””â”€â”€ targets/         # YAML configs per domain
src/
  â”œâ”€â”€ code/
  â”‚   â””â”€â”€ publicdata/  # Modular data processing scripts
  â””â”€â”€ data/
      â”œâ”€â”€ raw/         # Unmodified downloaded files
      â”œâ”€â”€ cleaned/     # Transformed and merged outputs
      â”œâ”€â”€ qc/          # Intermediate debug/QC files
      â””â”€â”€ metadata/    # Metadata logs and reports
```
```bash
src/code/                    # Core processing scripts
â”œâ”€â”€ publicdata/         # Domain-specific modules (targets, drugs, etc.)
â”‚   â””â”€â”€ target_data/    # e.g., ensembl_download.py, ncbi_transform.py...
â”‚   â””â”€â”€ disease_data/    # e.g., mondo_download.py, disease_merge.py...
src/data/                   # Input/output
â”‚   â””â”€â”€ target_data/
â”‚        â””â”€â”€ raw/                # Downloaded files (ignored in Git)
â”‚        â””â”€â”€ cleaned/            # Final curated outputs
â”‚            â””â”€â”€ sources/         # cleaned dataframes
â”‚        â””â”€â”€ metadata/               # Metadata & provenance
src/workflows/              # Snakemake workflows, cron scripts
src/tests/                  # Pytest unit tests per module
```

## ğŸ› ï¸ Usage
examples:
```bash
python src/scripts/main.py TARGETS --all
```
or 
```bash
python main.py TARGETS --ncbi_download
```
Or use Snakemake:

```bash
snakemake -s src/workflows/targets.Snakefile --cores 4
```

## ğŸ“¦ Dependencies

Install with pip:

```bash
pip install -r requirements.txt
```

Or via conda:

```bash
conda env create -f tgbuild.yml
```

## ğŸ“Š Outputs
- Raw source data downloads in `*data/raw/`
- Cleaned TSVs/CSVs in `*data/cleaned/`
- Metadata, logs, and diffs in `*data/metadata/`

## ğŸ“… Automate
Schedule with cron or run full DAG via Snakemake.

---

Â© 2025 NCATS_IFX
